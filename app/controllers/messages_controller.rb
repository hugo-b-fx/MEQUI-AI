class MessagesController < ApplicationController
  before_action :authenticate_user!

  def create
    @chat = current_user.chat || current_user.create_chat!

    content = params.dig(:message, :content)&.strip

    if content.blank?
      return head :unprocessable_entity
    end

    user_message = @chat.messages.create!(
      content: content,
      role: "user",
      user: current_user
    )

    messages_for_llm = @chat.messages.order(:created_at).last(15).map do |m|
      { role: m.role, content: m.content }
    end

    system_prompt = <<~PROMPT
      Tu es MequiA, l'assistant IA ultra-sympa et expert Ã©questre de mâ€™equi.
      Tu aides les cavaliers Ã  trouver le coach parfait en posant des questions intelligentes
      et en proposant des matchs personnalisÃ©s.
      Tu es chaleureux, tu tutoies, tu utilises des emojis ğŸâœ¨.

      Tu connais le profil de l'utilisateur :
      - Nom : #{current_user.name}
      - Niveau : #{current_user.level || "non renseignÃ©"}
      - Localisation : #{current_user.location || "non renseignÃ©e"}
      - Objectif : #{current_user.objective || "non renseignÃ©"}
      - Cheval : #{current_user.horses.first&.name || "non renseignÃ©"} (#{current_user.horses.first&.breed || ""})

      Pose des questions prÃ©cises pour affiner le matching et,
      quand tu as assez dâ€™infos, propose les 3 meilleurs coaches avec explication.
      RÃ©ponds toujours en franÃ§ais, de faÃ§on concise, claire et fun.
    PROMPT

    messages_for_llm.unshift({ role: "system", content: system_prompt })

    begin
      response = RubyLLM.chat(
        model: "gpt-4o-mini",
        messages: messages_for_llm,
        temperature: 0.7
      )

      ai_content = response.dig("choices", 0, "message", "content")
      ai_content ||= "Je nâ€™ai pas bien compris la rÃ©ponse du modÃ¨le, peux-tu reformuler ? ğŸ˜…"
    rescue StandardError => e
      ai_content = "Oupsâ€¦ une petite erreur technique est survenue ğŸ˜… Peux-tu rÃ©pÃ©ter ta question ?"
    end

    assistant_message = @chat.messages.create!(
      content: ai_content,
      role: "assistant"
    )

    respond_to do |format|
      format.turbo_stream
      format.html { redirect_to chat_path }
    end
  end
end
