class MessagesController < ApplicationController
  before_action :authenticate_user!

  def create
    @chat = current_user.chat || current_user.create_chat!

    content = params.dig(:message, :content)&.strip
    return head :unprocessable_entity if content.blank?

    user_message = @chat.messages.create!(
      content: content,
      role: "user",
      user: current_user
    )

    messages_for_llm = @chat.messages.order(:created_at).last(15).map do |m|
      { role: m.role, content: m.content }
    end

    system_prompt = <<~PROMPT
      Tu es MequiBotIA, l'assistant IA expert √©questre de m‚Äôequi üêé‚ú®
      Tu es chaleureux, tu tutoies, tu utilises des emojis.
      Tu connais le profil de l'utilisateur :
      ‚Ä¢ Nom : #{current_user.name}
      ‚Ä¢ Niveau : #{current_user.level || "non renseign√©"}
      ‚Ä¢ Localisation : #{current_user.location || "non renseign√©e"}
      ‚Ä¢ Objectif : #{current_user.objective || "non renseign√©"}
      ‚Ä¢ Cheval : #{current_user.horses.first&.name || "non indiqu√©"} (#{current_user.horses.first&.breed || ""})

      Pose des questions intelligentes pour affiner le matching coach.
<<<<<<< HEAD
      Quand tu as assez d‚Äôinfos, propose les 3 meilleurs avec explication personnalis√©e.
=======
      Quand tu as assez d‚Äôinfos, propose les 3 meilleurs coaches avec explication personnalis√©e.
>>>>>>> c2d3dae (fix: resolve merge conflicts)
      R√©ponds toujours en fran√ßais, sois fun, concis et motivant.
    PROMPT

    messages_for_llm.unshift({ role: "system", content: system_prompt })

    begin
      response = RubyLLM.chat(
        model: "gpt-4o-mini",
        messages: messages_for_llm,
        temperature: 0.7
      )
<<<<<<< HEAD
      ai_content = response.dig("choices", 0, "message", "content")
      ai_content ||= "Je r√©fl√©chis encore... üê¥ Peux-tu reformuler ?"
    rescue StandardError => e
      Rails.logger.error "RubyLLM error: #{e.message}"
      ai_content = "Oups, je suis un peu distrait aujourd‚Äôhui üòÖ Peux-tu r√©p√©ter ta question ?"
    end

    @chat.messages.create!(content: ai_content, role: "assistant")
=======

      ai_content = response.dig("choices", 0, "message", "content")
      ai_content ||= "Je n‚Äôai pas bien compris, peux-tu reformuler ? üòÖ"
    rescue StandardError => e
      Rails.logger.error "Erreur RubyLLM : #{e.message}"
      ai_content = "Oups, je suis un peu distrait aujourd‚Äôhui üòÖ Peux-tu r√©p√©ter ta question ?"
    end

    @chat.messages.create!(
      content: ai_content,
      role: "assistant"
    )
>>>>>>> c2d3dae (fix: resolve merge conflicts)

    respond_to do |format|
      format.turbo_stream
      format.html { redirect_to chat_path }
    end
  end

  private

  def message_params
    params.require(:message).permit(:content)
  end
end
